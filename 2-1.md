## Dynamic Programming
In the previous chapter, you learned how to write recursively and how to use recursion to solve some rather complex problems.

While recursion can certainly solve some problems, it can also create new ones if not used properly. In fact, recursion is often the culprit behind some of the slowest categories of Big O, such as `O(2N)`.

The good news, though, is that many of these problems can be avoided. 
In this chapter, you’ll learn how to identify some of the most common speed traps found in recursive code and how to express such algorithms in terms of Big O. 
More important, you’ll learn the techniques to fix these problems.

Here’s some more good news: the techniques found in this chapter are pretty simple. Let’s take a look at how to use these easy but effective methods for turning our recursive nightmares into recursive bliss.

### Unnecessary Recursive Calls
Here’s a recursive function that finds the greatest number from an array:
```
​ 	​function​ max(array) {
​ 	  ​if​ (array.length === 0) { ​return​ ​null​; }
​ 	
​ 	  ​if​ (array.length === 1) { ​return​ array[0]; }
​ 	
​ 	  ​if​ (array[0] > max(array.slice(1))) {
​ 	    ​return​ array[0];
​ 	  } ​else​ {
​ 	    ​return​ max(array.slice(1));
​ 	  }
​ 	}
```


The essence of each recursive call is the comparison of a single number (`array[0]`) to the maximum number from the remainder of the array. 
(To calculate the maximum number from the remainder of the array, we call the very max function we’re in, which is what makes the function recursive.)

We achieve the comparison with a conditional statement. The first half of the conditional statement is as follows:
```
​ 	​if​ (array[0] > max(array.slice(1))) {
​ 	  ​return​ array[0];
​ 	}
```


This snippet says that if the single number (`array[0]`) is greater than what has already been determined to be the maximum number of the rest of the array (`max(array.slice(1))`), then by definition, `array[0]` must be the greatest number, so we return it.

Here is the second half of the conditional statement:
```
​ 	​else​ {
​ 	  ​return​ max(array.slice(1));
​ 	}
```

This second snippet says that if array[0] is not greater than the greatest number from the rest of the array, then the greatest number from the rest of the array must be the greatest number overall, and we return it.

While this code works, it contains a hidden inefficiency. If you look carefully, you’ll note that our code contains the phrase, max(array.slice(1)) twice, once in each half of the conditional statement.

The problem with this is that each time we mention max(array.slice(1)), we trigger an entire avalanche of recursive calls.

Let’s break this down for an example array of [1, 2, 3, 4].

We know that we’re going to start by comparing the 1 with the maximum number of the remaining array, [2, 3, 4]. That, in turn, will compare the 2 against the max of the remaining [3, 4], which in turn will compare the 3 against the [4]. This, too, triggers one more recursive call on the [4] itself, which is the base case.

However, to really see how our code plays out, we’re going to start by analyzing the bottom call and working our way up the call chain.

Let’s begin.

### Max Recursive Walk-Through
When we call max([4]), the function simply returns the number 4. Again, this is because our base case is when the array only contains one element, as dictated by the following line of code:


```
 	​if​ (array.length === 1) {
​ 	  ​return​ array[0];
​ 	}
```

This is pretty straightforward—it’s a single function call:

<img width="900" height="50" alt="image" src="https://github.com/user-attachments/assets/447e6098-7b8b-4c9c-92dc-750b5225acb1" />

Moving up the call chain, let’s see what happens when we call max([3, 4]). In the first half of the conditional statement (if (array[0] > max(array.slice(1)))), we compare the 3 to max([4]). But calling max([4]) is itself a recursive call. The following diagram depicts max([3, 4]) calling max([4]):

<img width="900" height="165" alt="image" src="https://github.com/user-attachments/assets/2fa1b4c8-916c-45d5-aecf-45dc355c939b" />

Note that next to the arrow, we put the label “1st” to indicate that this recursive call was triggered by the first half of the conditional statement within max([3, 4]).

After this step has been completed, our code can now compare the 3 with the result of max([4]). Since the 3 is not greater than that result (4), we trigger the second half of the conditional. (This is the code, return max(array.slice(1)).) In this case, we return max([4]).

But when our code returns max([4]), it triggers the actual function call of max([4]). This is now the second time we’ve triggered the max([4]) call:


<img width="900" height="169" alt="image" src="https://github.com/user-attachments/assets/cfede404-0691-4f4f-942d-0b4138c8efa3" />

As you can see, the function, max([3, 4]) ends up calling max([4]) twice. Of course, we’d rather try to avoid doing this if we don’t have to. If we’ve already computed the result of max([4]) once, why should we call the same function again just to get the same result?

This problem gets a lot worse when we move just one level up the call chain.

Here’s what happens when we call max([2, 3, 4]).

During the first half of the conditional, we compare the 2 against max([3, 4]), which we’ve already determined looks like this:

<img width="900" height="169" alt="image" src="https://github.com/user-attachments/assets/adac3b08-ca87-45ec-a8d7-75805e401416" />


So max([2, 3, 4]) calling max([3, 4]) then, would look like this:

<img width="801" height="254" alt="image" src="https://github.com/user-attachments/assets/267e3c69-50ee-4ce1-b5c0-84748a1f4478" />

But here’s the kicker. This is just for the first half of the conditional of max([2, 3, 4]). For the second half of the conditional, we end up calling max([3, 4]) again:

<img width="801" height="261" alt="image" src="https://github.com/user-attachments/assets/b6f5313e-1b89-498c-8dff-c8b7900fbe35" />

Yikes!

Let’s dare to move to the top of the call chain, calling `max([1, 2, 3, 4])`. When all is said and done, after we call max for both halves of the conditional, we get what is shown in the diagram.

<img width="900" height="278" alt="image" src="https://github.com/user-attachments/assets/e9c2aee9-4a85-4fec-a856-7ba4764b2da9" />

So when we call `max([1, 2, 3, 4])`, we actually end up triggering the max function fifteen times.

We can see this visually by adding the statement, console.log(’RECURSION’) to the beginning of our function:
```
​ 	​function​ max(array) {
​ 	  console.log(​'RECURSION'​);
​ 	
​ 	  ​// remaining code omitted for brevity​
​ 	}

```


When we then run our code, we’ll see the word RECURSION printed to our terminal fifteen times.

Now, we do need some of those calls, but not all of them. We do need to calculate max([4]), for example, but one such function call is enough to get the computed result. But here, we call that function eight times.


### The Little Fix for Big O
Thankfully, there’s an easy way to eliminate all these extra recursive calls. We’ll call max only once within our code, and save the result to a variable:


```
 	​function​ max(array) {
​ 	  ​if​ (array.length === 0) { ​return​ ​null​; }
​ 	
​ 	  ​if​ (array.length === 1) { ​return​ array[0]; }
​ 	
​ 	  ​// Calculate the max of the remainder of the array​
​ 	  ​// and store it inside a variable:​
​ 	  ​const​ maxOfRemainder = max(array.slice(1));
​ 	
​ 	  ​// Comparison of first number against this variable:​
​ 	  ​if​ (array[0] > maxOfRemainder) {
​ 	    ​return​ array[0];
​ 	  } ​else​ {
​ 	    ​return​ maxOfRemainder;
​ 	  }
​ 	}
```

By implementing this simple modification, we end up calling max a mere four times. Try it out yourself by adding the console.log(’RECURSION’) line and running the code.

The trick here is we’re making each necessary function call once and saving the result in a variable so we don’t have to ever call that function again.

The difference in efficiency between our initial function and our ever-so-slightly modified function is stark.


### The Efficiency of Recursion
In our second, improved version of the max function, the function recursively calls itself as many times as there are values in the array. We’d call this O(N).

Up until this point, the cases of O(N) we’ve seen involved loops, with a loop running N times. However, we can apply the same principles of Big O to recursion as well.

As you’ll recall, Big O answers the key question: if there are N data elements, how many steps will the algorithm take?

Since the improved max function runs N times for N values in the array, it has a time complexity of O(N). Even if the function itself contains multiple steps, such as five, its time complexity would be O(5N), which is reduced to O(N).

In the first version, though, the function called itself twice during each run (save for the base case). Let’s see how this plays out for different array sizes.

The following table shows how many times max gets called on arrays of various sizes:

| N Elements | Number of Calls |

| 1 | 1 |

| 2 | 3 |

| 3 | 7 |

| 4 |  15 |

| 5 | 31 |


Can you see the pattern? When we increase the data by one, we roughly double the number of steps the algorithm takes. As you learned by our discussion of the ​Password Cracker​, this is the pattern of O(2N). We already know that this is an extremely slow algorithm.

The improved version of the max function, however, only calls max for as many elements as there are inside the array. This means that our second max function has an efficiency of O(N).

This is a powerful lesson: avoiding extra recursive calls is key to keeping recursion fast. What at first glance was a very small change to our code—the mere storing of a computation in a variable—ended up changing the speed of our function from O(2N) to O(N).


### Overlapping Subproblems
The Fibonacci sequence is a mathematical sequence of numbers that goes like this until infinity:
```
0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55…
```
This sequence begins with the numbers 0 and 1, and each subsequent number is the sum of the previous two numbers of the sequence. For example, the number 55 was computed because it is the sum of the previous two numbers, which are 21 and 34.

The following JavaScript function returns the Nth number in the Fibonacci sequence. For example, if we pass the number 10 to the function, it will return 55, as 55 is the tenth number in the series. (The 0 is considered the 0th number in the series.)

```
​ 	​function​ fib(n) {
​ 	  ​if​ (n === 0 || n === 1) {
​ 	    ​return​ n;
​ 	  }
​ 	
​ 	  ​return​ fib(n - 2) + fib(n - 1);
​ 	}
```

The key line in this function is the following:
```
​ 	​return​ fib(n - 2) + fib(n - 1);
```

This line sums the previous two numbers in the Fibonacci series. It’s a beautiful recursive function.

However, alarm bells should be going off in your head right now because our function calls itself twice.

Let’s take the computation of the sixth Fibonacci number, for example. The function fib(6) makes a call to both fib(4) and fib(5), as shown in the following diagram:

<img width="900" height="117" alt="image" src="https://github.com/user-attachments/assets/e0ac498f-6d51-4ce0-8ecd-96db2dd71813" />

As we’ve seen, a function calling itself twice can easily lead us down the road to O(2N). Indeed, all the recursive calls made by fib(6) are shown in the diagram.

<img width="900" height="436" alt="image" src="https://github.com/user-attachments/assets/b0d22d36-9962-4a48-98f6-09ba5f92094a" />

You’ve got to admit, O(2N) can look pretty scary.

While one simple change worked to optimize the first example in this chapter, optimizing our Fibonacci sequence isn’t as simple.

And that’s because there isn’t just one single piece of data we can save in a variable. We need to calculate both fib(n - 2) and fib(n - 1) (as each Fibonacci number is the sum of those two numbers), and storing the result of one won’t alone give us the result for the other.

This is a case of what computer scientists call overlapping subproblems. Let’s unpack that term.

When a problem is solved by solving smaller versions of the same problem, the smaller problem is called a subproblem. This concept isn’t new—we’ve been dealing with it frequently throughout our discussion of recursion. In the case of the Fibonacci sequence, we compute each number by first computing the smaller numbers in the sequence. The computations of these smaller numbers are the subproblems.

What makes these subproblems overlapping, though, is the fact that fib(n - 2) and fib(n - 1) end up calling many of the same functions as each other. Specifically, fib(n - 1) ends up making some of the very same calculations already made by fib(n - 2). For example, you can see in the previous diagram that fib(5) calls fib(3) even though fib(4) has itself already done so. Many other calls are duplicated too.

Well, it seems that we’re at a dead end. Our Fibonacci example requires us to make many overlapping function calls, and our algorithm oozes along at a pace of O(2N). There’s just nothing we can do.

Or is there?

### Dynamic Programming Through Memoization
Luckily, we do have options, and that is through something called dynamic programming. Dynamic programming is the process of optimizing recursive problems that have overlapping subproblems.

(Don’t pay too much attention to the word dynamic. There’s some debate as to how the term came about, and there’s nothing obviously dynamic about the techniques I’m about to demonstrate.)

Optimizing an algorithm with dynamic programming is typically accomplished with one of two techniques.

The first technique is something called memoization. And no, that’s not a typo. Pronounced meh-moe-ih-ZAY-shun, memoization is a simple, but brilliant, technique for reducing recursive calls in cases of overlapping subproblems.

Essentially, memoization reduces recursive calls by remembering previously computed functions. (In this respect, memoization really is like its similar-sounding word memorization.)

In our Fibonacci example, the first time fib(3) is called, the function does its computation and returns the number 2. However, before moving on, the function stores this result inside a hash table. The hash table will look something like this:

```
​ 	{3: 2}
```

This indicates that the result of fib(3) is the number 2.

Similarly, our code will memoize the results of all new computations it encounters. After encountering fib(4), fib(5), and fib(6), for example, our hash table will look like this:
```
​ 	{
​ 	  3: 2,
​ 	  4: 3,
​ 	  5: 5,
​ 	  6: 8
​ 	}
```

Now that we have this hash table, we can use it to prevent future recursive calls. Here’s the way this works:

Without memoization, fib(4) would normally call fib(3) and fib(2), which in turn make their own recursive calls. Now that we have this hash table, we can approach things differently. Instead of fib(4) just blithely calling fib(3), for example, it first checks the hash table to see if the result of fib(3) has already been computed. Only if the 3 key is not in the hash table does the function proceed to call fib(3).

Memoization goes for the jugular of overlapping subproblems. The whole issue with overlapping subproblems is that we end up computing the same recursive calls over and over again. With memoization, though, each time we make a new calculation, we store it in the hash table for future use. This way, we only make a calculation if it hasn’t ever been made before.

Okay, this all sounds good, but there’s one glaring problem. How does each recursive function get access to this hash table?

The answer is this: we pass the hash table as a second parameter to the function.

Because the hash table is a specific object in memory, we’re able to pass it from one recursive call to the next, even though we’re modifying it as we go. This is true even as we unwind the call stack. Even though the hash table may have been empty when the original call was made, that same hash table can be full of data by the time the original call has finished executing.

### Implementing Memoization
To pass the hash table along, we modify our function to accept two arguments, with the hash table as the second. We call this hash table, memo, as in memoization:
```
​ 	​function​ fib(n, memo={}) {
```

Note that we’re setting memo up as a default argument; this way we don’t have to pass in an empty hash table the first time we call it. As such, we can simply call the function using this:
```
​ 	fib(6);
```

Each time fib calls itself, it also passes along the hash table, which gets filled up along the way.

Here’s the rest of the function:

```
	​function​ fib(n, memo={}) {
​ 	  ​if​ (n === 0 || n === 1) {
​ 	    ​return​ n;
​ 	  }
​ 	
​ 	  ​if​ (!memo[n]) {
​ 	    memo[n] = fib(n - 2, memo) + fib(n - 1, memo);
​ 	  }
​ 	
​ 	  ​return​ memo[n];
​ 	}
```

Let’s analyze this line by line.

Again, our function now accepts two parameters, namely n and the memo hash table:

```
	​function​ fib(n, memo={}) {
```

First off, the base cases of 0 and 1 both automatically return n and are unaffected by memoization.

Before making any recursive calls, our code first checks to see whether fib(n) has already been calculated for the given n:
```
​ 	​if​ (!memo[n]) {
```

(If the calculation for n is already in the hash table, we simply return the result with return memo[n].)

Only if the calculation for n has not yet been made do we proceed with the calculation:
```
​ 	memo[n] = fib(n - 2, memo) + fib(n - 1, memo);
```

Here, we store the result of the calculation in the memo hash table so we never have to calculate it again.

Also note how we pass memo along as an argument to the fib function each time we call it. This is the key to sharing the memo hash table across all the calls to the fib function.

As you can see, the guts of the algorithm remain the same. We’re still using recursion to solve our problem, as the computation of fib is still essentially fib(n - 2) + fib(n - 1). However, if the number we’re computing is new, we store the result in a hash table, and if the number we’re computing was already computed once before, we simply grab the result from the hash table instead of computing it again.

When we map out the recursive calls in our memoized version, we get this:


<img width="900" height="297" alt="image" src="https://github.com/user-attachments/assets/687ba3b0-85a6-4aec-9b79-7a95e3fee4ed" />

In this diagram, each call that is surrounded by a box is one in which the result was retrieved from the hash table.

So what is the Big O of our function now? Let’s look at how many recursive calls we make for different types of N:

N Elements

Number of Calls

1 | 1

2 | 3

3 | 5

4 | 7

5 | 9

6 | 11

We can see that for N, we make (2N) - 1 calls. Since in Big O we drop the constants, this is an O(N) algorithm.

This is an incredible improvement over O(2N). Go memoization!

### Dynamic Programming Through Going Bottom-Up
I mentioned earlier that dynamic programming can be achieved through one of two techniques. We looked at one technique, memoization, which is quite nifty.

The second technique, known as going bottom-up, is a lot less fancy and may not even seem like a technique at all. All going bottom-up means is to ditch recursion and use some other approach (like a loop) to solve the same problem.

The reason that going bottom-up is considered part of dynamic programming is because dynamic programming means taking a problem that could be solved recursively and ensuring that it doesn’t make duplicate calls for overlapping subproblems. Using iteration (that is, loops) instead of recursion is, technically, a way to achieve this.


Going bottom-up becomes more of a “technique” when the problem is more naturally solved with recursion. Generating Fibonacci numbers is one example where recursion is a neat, elegant solution. Having to solve the same problem with iteration may take more brainpower, as an iterative approach may be less intuitive. (Imagine solving the staircase problem from the previous chapter with a loop. Ugh.)

Let’s see how we might implement a bottom-up approach for our Fibonacci function.

### Bottom-Up Fibonacci
In the following bottom-up approach, we start with the first two Fibonacci numbers: 0 and 1. We then use good ol’ iteration to build up the sequence:

```
 	​function​ fib(n) {
​ 	  ​if​ (n === 0) { ​return​ 0; }
​ 	
​ 	  ​let​ a = 0;
​ 	  ​let​ b = 1;
​ 	
​ 	  ​for​ (​let​ i = 1; i < n; i += 1) {
​ 	    [a, b] = [b, a + b];
​ 	  }
​ 	
​ 	  ​return​ b;
​ 	}
```

The very first thing we do is return 0 if the input n is 0. Officially, fib(0) should return 0, so we easily dispense with that with a simple conditional statement.

Now, we get to the meat of the code. We initialize variables a and b as 0 and 1 respectively, as those are the first two numbers of the Fibonacci sequence.

We then start a loop so we can calculate each number of the sequence until we reach n:
```
​ 	​for​ (​let​ i = 1; i < n; i += 1) {
```


To calculate the next number in the sequence, we need to add the two previous numbers together, namely a + b. We’ll update b to be this new number, since b always is supposed to point to the last number in the sequence. We also update a to what b was, so that a continues to point to the second-to-last number. We do this all with one convenient swap:
```
​ 	[a, b] = [b, a + b];
```

Because our code is a simple loop from 1 to N, our code takes N steps. Like the memoization approach, it’s O(N).

### Memoization vs. Bottom-Up
You’ve now seen the two primary techniques of dynamic programming: memoization and going bottom-up. Is one technique better than the other?

Usually, it depends on the problem and why you’re using recursion in the first place. If recursion presents an elegant and intuitive solution to a given problem, you may want to stick with it and use memoization to deal with any overlapping subproblems. However, if the iterative approach is equally intuitive, you may want to go with that.

It’s important to point out that even with memoization, recursion does carry some extra overhead versus iteration. Specifically, with any recursion, the computer needs to keep track of all the calls in a call stack, which consumes memory. The memoization itself also requires the use of a hash table, which will take up additional space on your computer as well. (More on this in Chapter 19, ​Dealing with Space Constraints​.)

Generally speaking, going bottom-up is often the better choice, unless the recursive solution is more intuitive. Where recursion is more intuitive, you can keep the recursion and keep it fast by using memoization.


### Wrapping Up
Now that you’re able to write efficient recursive code, you’ve also unlocked a superpower. You’re about to encounter some really efficient—yet advanced—algorithms, and many of them rely on the principles of recursion.

### Exercises
The following exercises provide you with the opportunity to practice with dynamic programming. The solutions to these exercises are found in the section ​Chapter 12​.

1. The following function accepts an array of numbers and returns the sum, as long as a particular number doesn’t bring the sum above 100. If adding a particular number will make the sum higher than 100, that number is ignored. However, this function makes unnecessary recursive calls. Fix the code to eliminate the unnecessary recursion:

```
 	​function​ addUntil100(array) {
​ 	  ​if​ (array.length === 0) { ​return​ 0; }
​ 	
​ 	  ​if​ (array[0] + addUntil100(array.slice(1)) > 100) {
​ 	    ​return​ addUntil100(array.slice(1));
​ 	  } ​else​ {
​ 	    ​return​ array[0] + addUntil100(array.slice(1));
​ 	  }
​ 	}
```


2. The following function uses recursion to calculate the Nth number from a mathematical sequence known as the Golomb sequence. It’s terribly inefficient, though! Use memoization to optimize it. (You don’t have to understand how the Golomb sequence works to do this exercise.)
```
​ 	​function​ golomb(n) {
​ 	  ​if​ (n === 1) { ​return​ 1; }
​ 	
​ 	  ​return​ 1 + golomb(n - golomb(golomb(n - 1)));
​ 	}
```

3. Here is a solution to the unique paths problem from an exercise in the previous chapter. (Sorry, it’s a bit of a spoiler if you haven’t tried doing that exercise yet.) Use memoization to improve its efficiency:
```
​ 	​function​ uniquePaths(rows, columns) {
​ 	  ​if​ (rows === 1 || columns === 1) {
​ 	    ​return​ 1;
​ 	  }
​ 	
​ 	  ​return​ uniquePaths(rows - 1, columns) + uniquePaths(rows, columns - 1);
​ 	}
```



### Solution

1. The problem here is the function recursively calls itself twice each time it runs. Let’s make it so that it only calls itself once each time:
```
​ 	​function​ addUntil100(array) {
​ 	  ​if​ (array.length === 0) { ​return​ 0; }
​ 	
​ 	  ​const​ sumOfRemainingNumbers = addUntil100(array.slice(1));
​ 	
​ 	  ​if​ (array[0] + sumOfRemainingNumbers > 100) {
​ 	    ​return​ sumOfRemainingNumbers;
​ 	  } ​else​ {
​ 	    ​return​ array[0] + sumOfRemainingNumbers;
​ 	  }
​ 	}
```

2. Here is the memoized version:
```
​ 	​function​ golomb(n, memo = {}) {
​ 	  ​if​ (n === 1) { ​return​ 1; }
​ 	
​ 	  ​if​ (!memo[n]) {
​ 	    memo[n] = 1 + golomb(n - golomb(golomb(n - 1, memo), memo), memo);
​ 	  }
​ 	
​ 	  ​return​ memo[n];
​ 	}
```

3. To accomplish memoization here, we need to make a key that takes into account both the number of rows and the number of columns. To this end, we can make our key be based on the row and column together.
```
​ 	​function​ uniquePaths(rows, columns, memo = {}) {
​ 	  ​if​ (rows === 1 || columns === 1) {
​ 	    ​return​ 1;
​ 	  }
​ 	
​ 	  ​if​ (!memo[[rows, columns]]) {
​ 	    memo[[rows, columns]] = (uniquePaths(rows - 1, columns, memo)
​ 	    + uniquePaths(rows, columns - 1, memo));
​ 	  }
​ 	
​ 	  ​return​ memo[[rows, columns]];
​ 	}
```


